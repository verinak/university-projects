# -*- coding: utf-8 -*-
"""1_Preprocessing.ipynb

Automatically generated by Colab.

The rise of Internet of Things (IoT) devices has introduced new security challenges, with IoT networks becoming targets for cyber-attacks.To address this, effective intrusion detection systems (IDS) are crucial. The IoTID20 dataset provides a valuable resource for studying IoT attacks. However, its high dimensionality poses computational challenges, especially for resource-constrained IoT devices. Dimensionality reduction techniques like PCA and LDA can help mitigate this challenge, making machine learning algorithms feasible for deployment. This project explores the impact of dimensionality reduction on SVM classifiers using the IoTID20 dataset, aiming to balance computational efficiency and classification accuracy. It also proposes a novel technique for optimizing this balance. Ultimately, this project contributes to enhancing IoT security by providing insights into machine learning-based intrusion detection systems and their practical deployment in IoT environments.
"""

import pandas as pd
import numpy as np
import struct, socket, warnings

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.pipeline import Pipeline

warnings.filterwarnings("ignore")  # Ignore all warnings
pd.set_option('display.max_columns', None)  # Display all columns

"""# I - Read Data"""

from google.colab import drive
drive.mount('/content/drive')
dir_path = '/content/drive/MyDrive/computation data/'

data_path = dir_path + 'IoT Network Intrusion Dataset.csv'
# data_path = r'data/IoT Network Intrusion Dataset.csv'

df = pd.read_csv(data_path)
df.head(15)

"""# II - EDA"""

df.info()

"""## Data Visualization

### Exploring target data
"""

import seaborn as sns
import matplotlib.pyplot as plt



fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Plotting the distribution of the target variable 'label'
sns.countplot(data=df, x='Label', palette='pastel', ax=axes[0])
axes[0].set_title('Distribution of Target Variable (Label)')
axes[0].set_xlabel('Label')
axes[0].set_ylabel('Frequency')

# Plotting the distribution of the target variable 'Cat'
sns.countplot(data=df, x='Cat', palette='pastel', ax=axes[1])
axes[1].set_title('Distribution of Target Variable (Category)')
axes[1].set_xlabel('Category')
axes[1].set_ylabel('Frequency')

# Plotting the distribution of the target variable 'Sub_Cat'
sns.countplot(data=df, x='Sub_Cat', palette='pastel', ax=axes[2])
axes[2].set_title('Distribution of Target Variable (SubCategory)')
axes[2].set_xlabel('SubCategory')
axes[2].set_ylabel('Frequency')

# Rotate labels vertically for the 'Sub_Cat' plot
axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=90)

plt.tight_layout()
plt.show()

"""### Protocol Number Distribution"""

# Count the frequency of each protocol
protocol_counts = df['Protocol'].value_counts()

# Extract the protocol labels from the DataFrame
labels = df['Protocol'].unique()

# Define protocol names
protocols = {'0': 'HOPOPT', '6': 'TCP', '17': 'UDP'}

# Add text to labels
labels_with_text = [f'Protocol {label} ({protocols[str(label)]})' for label in labels]

# Define custom pastel colors for the pie chart
colors = ['#a1caf1', '#b2e0b2', '#ffb3e6']  # Pastel blue, pastel green, and pastel pink

# Create a pie chart to visualize the distribution of protocols
plt.figure(figsize=(6, 6))
plt.pie(protocol_counts, labels=labels_with_text, autopct='%1.1f%%', startangle=140, colors=colors)
plt.title('Distribution of Protocols')
plt.show()

"""###  Top 10 most frequent source and Destination IP addresses."""

fig, axes = plt.subplots(1, 2, figsize=(18, 6))

# Bar plot showing the frequency of each source IP address
sns.countplot(df['Src_IP'].astype(str), order=(df['Src_IP'].astype(str)).value_counts().index[:10], ax=axes[0], color='navy')
axes[0].set_title('Top 10 Most Frequent Source IP Addresses')
axes[0].set_xlabel('Frequency')
axes[0].set_ylabel('Source IP Address')
axes[0].tick_params(axis='x', rotation=45)

# Bar plot showing the frequency of each destination IP address
sns.countplot(df['Dst_IP'].astype(str), order=(df['Dst_IP'].astype(str)).value_counts().index[:10], ax=axes[1], color='navy')
axes[1].set_title('Top 10 Most Frequent Destination IP Addresses')
axes[1].set_xlabel('Frequency')
axes[1].set_ylabel('Destination IP Address')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Bar plot to show the count or mean of a variable for different categories
sns.barplot(data=df, x='Sub_Cat', y='Flow_Duration', estimator=sum)
plt.title('Bar Plot of Sum of Flow_Duration by subCategory')
plt.xlabel('subCategory')
plt.ylabel('Sum of Flow_Duration')
plt.xticks(rotation=45)
plt.show()

"""# III - Data Cleaning

## Check for duplicates and null values
"""

for column in df.columns:
    null_count = df[column].isnull().sum()
    if null_count > 0:
        print(f"Column '{column}' has {null_count} null values.")
print("done")

# To find duplicate rows
duplicate_rows = df[df.duplicated()]

# Remove columns with all NaN values
print("Remove columns with NaN values ... ", end="")
null_cols = df.columns[df.isnull().all()].tolist()
df = df.drop(null_cols, axis=1)
print("Done")
#note no nan columns

print("Convert IP addresses and dates to numerical data ... ", end="")

# Convert IP addresses to 32-bits integers
def convert_ip_to_int(ip):
    """
    Convert IP address to a 32-bit integer.
    Args:
        ip (str): IP address in string format.
    Returns:
        int: 32-bit integer representation of the IP address.
    """
    return struct.unpack("!I", socket.inet_aton(ip))[0]

df["Src_IP"] = df["Src_IP"].apply(convert_ip_to_int)
df["Dst_IP"] = df["Dst_IP"].apply(convert_ip_to_int)

# Convert Timestamp to 64-bits integers
df["Timestamp"] = pd.to_datetime(df["Timestamp"]).astype(np.int64)

print("Done")

"""## X and Y separation

### X
"""

# Keep only numerical data
dataframe = df.select_dtypes(include="number")
dataframe

"""### Replace infinities"""

# Replace infinities with column max or min depending on the sign
print("Replace infinities by column max ... ", end="")
# Replace positive infinities with column max
dataframe = dataframe.replace(np.inf, np.nan)
dataframe = dataframe.fillna(dataframe.max())
# Replace negative infinities with column min
dataframe = dataframe.replace(-np.inf, np.nan)
dataframe = dataframe.fillna(dataframe.min())
print("Done")

"""### X correlation with target variable"""

X=dataframe
y=df['Label']
y = y.map({'Anomaly': 1, 'Normal': 0})

# Compute the correlation between features and the target variable
correlation_with_label = X.corrwith(y)

# Sort the correlation values
correlation_with_label = correlation_with_label.abs().sort_values(ascending=False)

# Select top N features with highest correlation
N =20  # Number of top features to select
important_features = correlation_with_label.head(N).index.tolist()

# Subset the dataset with important features
data_subset = X[important_features]

print("Selected important features:", important_features)

# Drop the columns that do not logically affect the label
columns_to_drop = ['Timestamp', 'Dst_Port', 'Src_Port', 'Protocol', 'Src_IP']
data_subset.drop(columns=columns_to_drop, inplace=True, errors='ignore')
data_subset.columns

# Heatmap to visualize correlations between features
plt.figure(figsize=(12, 6))
sns.heatmap(data_subset.corr(), annot=True, cmap='magma', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# List of numerical columns
numerical_columns = ['Fwd_Pkt_Len_Std', 'ACK_Flag_Cnt', 'Init_Bwd_Win_Byts',
                     'Pkt_Len_Std', 'TotLen_Fwd_Pkts', 'Subflow_Fwd_Byts',
                     'Fwd_Pkt_Len_Max', 'Pkt_Len_Var', 'Bwd_Header_Len',
                     'Fwd_Header_Len', 'Fwd_Seg_Size_Avg']

# Create a new DataFrame containing only the specified numerical columns
numerical_df = df[numerical_columns]

# Sum the values for each numerical column by 'Sub_Cat'
sum_numerical_df = numerical_df.groupby(df['Sub_Cat']).sum()

# Create subplots
fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(18, 14))
fig.subplots_adjust(hspace=0.5)

axes = axes.flatten()

# Plot each numerical column
for i, column in enumerate(numerical_columns):
    sns.barplot(data=sum_numerical_df, x=sum_numerical_df.index, y=column, ax=axes[i])
    axes[i].set_title(f'Sum of {column} by subCategory')
    axes[i].set_xlabel('subCategory')
    axes[i].set_ylabel(f'Sum of {column}')
    axes[i].tick_params(axis='x', rotation=45)

for i in range(len(numerical_columns), len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.show()

"""### Min/Max scaling"""

# Scale the data to range [0, 1]
dataframe = (dataframe.max() - dataframe) / (dataframe.max() - dataframe.min())
print("Done")

# Remove columns with all NaN values
print("Remove columns with NaN values ... ", end="")
null_cols = dataframe.columns[dataframe.isnull().all()].tolist()
dataframe = dataframe.drop(null_cols, axis=1)
print("Done")

nan_values = dataframe.isna().any()

# Print columns with NaN values
print("Columns with NaN values:")
print(nan_values[nan_values].index)

"""### Y"""

# Extract the labeling features from the dataframe
binary_labels = df.Label
category_labels = df.Cat
subcategory_labels = df.Sub_Cat
 # One-hot encode for the labels
print("One-hot encoding the labels ... ", end="")
binary_y = binary_labels.replace({"Normal": 0, "Anomaly": 1}).to_numpy(copy=True)
category_y = category_labels.replace({"Normal": 0, "Mirai": 1, "DoS": 2, "Scan": 3,
                                              "MITM ARP Spoofing": 4}).to_numpy(copy=True)
subcategory_y = subcategory_labels.replace(
            {"Normal": 0, "Mirai-Ackflooding": 1, "Mirai-Hostbruteforceg": 2, "Mirai-UDP Flooding": 3,
             "Mirai-HTTP Flooding": 4, "DoS-Synflooding": 5, "Scan Port OS": 6, "Scan Hostport": 7,
             "MITM ARP Spoofing": 8}).to_numpy(copy=True)
df["Label"] = binary_y
df["Cat"] = category_y
df["Sub_Cat"] = subcategory_y
print("Done")

dataframe

data_path = dir_path + 'data_cleaned.csv'
# data_path = r'data/data_cleaned.csv'

df.to_csv(data_path, index=False)

data_path = dir_path + 'x_cleaned.csv'
# data_path = r'data/x_cleaned.csv'

dataframe.to_csv(data_path, index=False)

"""# IV - Dimentionality Reduction

## PCA

### Calculate total variance
"""

# Calculate variances of each column
variances = dataframe.var()

# Sort variances in ascending order
sorted_variances = variances.sort_values()

# Plot sorted variances
plt.figure(figsize=(10, 6))
sorted_variances.plot(kind='bar', color='skyblue')
plt.title('Variance of Columns (Sorted)')
plt.xlabel('Columns')
plt.ylabel('Variance')
plt.xticks(rotation=90)  # Rotating labels vertically
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


# Calculate variances of each column
variances = dataframe.var()

# Sort variances in ascending order
sorted_variances = variances.sort_values()

# Filter variances greater than 00.1
filtered_variances = sorted_variances[sorted_variances > 0.001]

# Plot filtered variances
plt.figure(figsize=(10, 6))
filtered_variances.plot(kind='bar', color='skyblue')
plt.title('Variance of Columns > 00.1 (Sorted)')
plt.xlabel('Columns')
plt.ylabel('Variance')
plt.xticks(rotation=90)  # Rotating labels vertically
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""#### We notice that before performing any pca dimensionality reduction that the MAX variance represented by a column = 0.25 , and that aproximately only 32 columns contribute in the variance of the data . Then max number of pca eigenvectors for this data = 32 , min number of pca eigen vectors = 2"""

# Filter original DataFrame based on columns with variances > 0.1
filtered_data = dataframe[filtered_variances.index]

# Display the new DataFrame
filtered_data

#drop Flow_ID, Src_IP, Dst_IP, Src_Port, Dst_Port, Timestamp

dataframe

"""### Perform PCA"""

preprocessed_data = dataframe.to_numpy()

cov_matrix = np.cov(preprocessed_data, rowvar=False)
# Calculate the sum of the main diagonal elements
diagonal_sum = np.trace(cov_matrix)

print("Sum of Main Diagonal Elements - Total variance captured by the data :", diagonal_sum)

print("Applying PCA to preprocessed data ... ", end="")
pca = PCA()
pca.fit(preprocessed_data)
pca_features = pca.transform(preprocessed_data)

num_pca_features = pca_features.shape[1]
print("Number of PCA Features:", num_pca_features)
print("Done")

# Get the eigenvalues
eigenvalues = pca.explained_variance_

# Get the sum of eigenvalues
sum_eigenvalues = np.sum(eigenvalues)

print("Eigenvalues:", eigenvalues)
print("Sum of Eigenvalues:", sum_eigenvalues)

import matplotlib.pyplot as plt


# Plot the eigenvalues
plt.figure(figsize=(8, 6))
plt.bar(range(len(eigenvalues)), eigenvalues, color='skyblue')
plt.title('Eigenvalues')
plt.xlabel('Principal Component Index')
plt.ylabel('Eigenvalue')
plt.grid(True)
plt.show()

# Print the sum of eigenvalues
print("Sum of Eigenvalues:", sum_eigenvalues)

"""#### we conclude that despite the error is approximately 0 but we still didn t achieve our dimensionality reduction goal , based on this graph we can conclude that only the first 14-15 eigen values capture all the variance of the data, then number of pca componenets must be changed

#### Scree plot as a proof of our conclusion:
"""

# Plotting the scree plot
plt.figure(figsize=(9, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='-')
plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.grid(True)
plt.show()

"""### LDA"""

print("Applying LDA to preprocessed data based on Label column... ", end="")
lda_label = LinearDiscriminantAnalysis()
lda_label.fit(preprocessed_data, df['Label'])
lda_features_label = lda_label.transform(preprocessed_data)
print("Done")
print(lda_features_label)

print("Applying LDA to preprocessed data based on Category column... ", end="")
lda_category = LinearDiscriminantAnalysis()
lda_category.fit(preprocessed_data, df['Cat'])
lda_features_category = lda_category.transform(preprocessed_data)
print("Done")
print(lda_features_category)

print("Applying LDA to preprocessed data based on Subcategory column... ", end="")
lda_subcategory = LinearDiscriminantAnalysis()
lda_subcategory.fit(preprocessed_data, df['Sub_Cat'])
lda_features_subcategory = lda_subcategory.transform(preprocessed_data)
print("Done")
print(lda_features_subcategory)

"""### PCA + LDA

#### Get Top 14 LDA columns
"""

# Get the coefficients of the linear discriminant function
feature_coefficients = lda_label.coef_[0]

# Create a dictionary to associate each feature name with its coefficient
feature_coefficients_dict = dict(zip(dataframe.columns, feature_coefficients))

# Calculate the absolute values of the coefficients
abs_feature_coefficients_dict = {feature: abs(coef) for feature, coef in feature_coefficients_dict.items()}

# Sort the coefficients by their absolute values
sorted_abs_coefficients = sorted(abs_feature_coefficients_dict.items(), key=lambda x: x[1], reverse=True)

# Extract feature names and absolute coefficients for plotting
feature_names_sorted = [item[0] for item in sorted_abs_coefficients]
abs_coefficients_sorted = [item[1] for item in sorted_abs_coefficients]

# Plot the bar chart
plt.figure(figsize=(10, 8))
plt.bar(np.arange(len(feature_names_sorted)), abs_coefficients_sorted, color='skyblue')
plt.xticks(np.arange(len(feature_names_sorted)), feature_names_sorted, rotation=90)
plt.ylabel('Absolute Coefficient Value')
plt.xlabel('Feature')
plt.title('Absolute Coefficients of Linear Discriminant Analysis')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Plot the bar chart
plt.figure(figsize=(10, 8))
plt.bar(np.arange(len(feature_names_sorted[:14])), abs_coefficients_sorted[:14], color='skyblue')
plt.xticks(np.arange(len(feature_names_sorted[:14])), feature_names_sorted[:14], rotation=90)
plt.ylabel('Absolute Coefficient Value')
plt.xlabel('Feature')
plt.title('Absolute Coefficients of Linear Discriminant Analysis (First 14)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Extract the top 14 feature names
top_14_feature_names = [item[0] for item in sorted_abs_coefficients[:14]]

# Create a DataFrame with the top 14 feature names
top_14_features_df = pd.DataFrame({'Feature Name': top_14_feature_names})

top_14_features_df

"""#### Get Top 30 PCA columns"""

# Calculate the covariance matrix
cov_matrix = np.cov(preprocessed_data, rowvar=False)

# Perform eigendecomposition on the covariance matrix
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Sort the eigenvalues in descending order and get their indices
sorted_indices = np.argsort(eigenvalues)[::-1]

# Get the indices of the top 30 eigenvalues
top_30_indices = sorted_indices[:30]

# Get the corresponding eigenvectors (principal components)
top_30_eigenvectors = eigenvectors[:, top_30_indices]

# Get the names of the features corresponding to the top 14 eigenvectors
top_30_feature_names = dataframe.columns[top_30_indices]


top_30_components_df = pd.DataFrame({'Feature Name': top_30_feature_names})

print("DataFrame with the top 30 feature names:")
top_30_components_df

"""#### Get common columns to use for PCA and LDA"""

# Merge the DataFrames on the 'Feature Name' column to find common elements
common_features_df = pd.merge(top_30_components_df, top_14_features_df, on='Feature Name')

common_features_df

"""### Combine Top 11 PCA features with Top 5 LDA features"""

top_11_pca = top_30_components_df[:11]['Feature Name'].to_list()
top_5_lda = top_14_features_df[:5]['Feature Name'].to_list()
combined_15_features = list(set(top_11_pca) | (set(top_5_lda)))
len(combined_15_features)

preprocessed_15 = dataframe[combined_15_features].to_numpy()
pca = PCA()
pca.fit(preprocessed_15)
pca_top_15 = pca.transform(preprocessed_15)
pca_top_15.shape

"""#### Perform PCA and LDA

#### We will define a function that performs PCA with a given number of components followed by LDA to avoid redundancy in our code
"""

def perform_pca_lda(y, X=pca_features, n_components=72):
  pca_data = X[:, :n_components]

  lda = LinearDiscriminantAnalysis()
  lda.fit(pca_data, y)
  pca_lda_data = lda.transform(pca_data)

  return pca_lda_data

def print_pca_lda_results(pca_lda_data):
  print("Shape of transformed data:", pca_lda_data.shape)

  print("Transformed data:")
  print(pca_lda_data)

"""We will call this function in the next files during model training while trying to determine the best combination of PCA features and LDA y variable"""