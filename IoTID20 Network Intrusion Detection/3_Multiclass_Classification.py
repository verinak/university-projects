# -*- coding: utf-8 -*-
"""3_Multiclass_Classification.ipynb

Automatically generated by Colab.

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings, time

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, cross_validate, KFold
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score
from sklearn.calibration import CalibratedClassifierCV

# warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)

"""# Loading Data

### Load data after cleaning
"""

from google.colab import drive
drive.mount('/content/drive')
dir_path = '/content/drive/MyDrive/computation data/'

data_path = dir_path + 'data_cleaned.csv'
# data_path = r'data/data_cleaned.csv'

df = pd.read_csv(data_path)

data_path = dir_path + 'x_cleaned.csv'
# data_path = r'data/x_cleaned.csv'

dataframe = pd.read_csv(data_path)
preprocessed_data = dataframe.to_numpy()

"""### Apply PCA and LDA to cleaned data"""

print("Applying PCA to preprocessed data ... ", end="")
pca = PCA()
pca.fit(preprocessed_data)
pca_features = pca.transform(preprocessed_data)

num_pca_features = pca_features.shape[1]
print("Number of PCA Features:", num_pca_features)
print("Done")

print("Applying LDA to preprocessed data based on Label column... ", end="")
lda_label = LinearDiscriminantAnalysis()
lda_label.fit(preprocessed_data, df['Label'])
lda_features_label = lda_label.transform(preprocessed_data)
print("Done")
print(lda_features_label)

print("Applying LDA to preprocessed data based on Category column... ", end="")
lda_category = LinearDiscriminantAnalysis()
lda_category.fit(preprocessed_data, df['Cat'])
lda_features_category = lda_category.transform(preprocessed_data)
print("Done")
print(lda_features_category)

print("Applying LDA to preprocessed data based on Subcategory column... ", end="")
lda_subcategory = LinearDiscriminantAnalysis()
lda_subcategory.fit(preprocessed_data, df['Sub_Cat'])
lda_features_subcategory = lda_subcategory.transform(preprocessed_data)
print("Done")
print(lda_features_subcategory)

"""#### We will define a function that performs PCA with a given number of components followed by LDA to avoid redundancy in our code"""

def perform_pca_lda(y, X=pca_features, n_components=72):
  pca_data = X[:, :n_components]

  lda = LinearDiscriminantAnalysis()
  lda.fit(pca_data, y)
  pca_lda_data = lda.transform(pca_data)

  return pca_lda_data

def print_pca_lda_results(pca_lda_data):
  print("Shape of transformed data:", pca_lda_data.shape)

  print("Transformed data:")
  print(pca_lda_data)

"""# Model Development

#### We will first define a function that trains the SVM model to avoid redundancy in our code
"""

def train_LinearSVC_Model(X, y, penalty='l2', loss='hinge', C=1.0, max_iter=100):

    """
    Implements training and testing of LinearSVC model with an 80/20 train-test split.

    Parameters:
        X: DataFrame of X features.
        y: Series of y variable.
        parameters that can be passed to LinearSVC function are added as optional parameters.
        check LinearSVC official documentation for more details.

    Returns:
        classifier: trained LinearSVC classifier.
        metrics: dictionary of evaluation metrics.
        conf_mat: ndarray of confusion matrix.
        precision_recall: list of precision-recall curve data.
    """

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    classifier = LinearSVC(penalty=penalty, loss=loss, max_iter=max_iter, C=C)

    # Train the classifier
    start_time = time.time()
    classifier.fit(X_train, y_train)
    fit_time = time.time() - start_time

    # Predict on the test set
    start_time = time.time()
    y_pred = classifier.predict(X_test)
    predict_time = time.time() - start_time

    # Calculate evaluation metrics

    conf_mat = confusion_matrix(y_test, y_pred)

    # Calibrate the classifier for probability estimation
    calibrated_classifier = CalibratedClassifierCV(classifier, method='sigmoid', cv='prefit')
    calibrated_classifier.fit(X_train, y_train)
    y_scores = calibrated_classifier.predict_proba(X_test)

    # Compute precision-recall pairs for each class
    precision = dict()
    recall = dict()
    average_precision = dict()
    for i in range(len(set(y))):
        precision[i], recall[i], _ = precision_recall_curve(y_test == i, y_scores[:, i])
        average_precision[i] = average_precision_score(y_test == i, y_scores[:, i])
    precision_recall = [precision, recall, average_precision]

    metrics = {
         'type':'train-test',
          'n_classes':len(set(y)),
          'accuracy': round(accuracy_score(y_test, y_pred) * 100, 2),
          'fit_time': round(fit_time  * 1000),
          'predict_time': round(predict_time  * 1000),
          'misclassified': np.sum(conf_mat) - np.sum(np.diag(conf_mat)),
          'precision_micro': round(precision_score(y_test, y_pred, average='micro'), 3),
          'precision_macro': round(precision_score(y_test, y_pred, average='macro'), 3),
          'precision_weighted': round(precision_score(y_test, y_pred, average='weighted'), 3),
          'recall_micro': round(recall_score(y_test, y_pred, average='micro'), 3),
          'recall_macro': round(recall_score(y_test, y_pred, average='macro'), 3),
          'recall_weighted': round(recall_score(y_test, y_pred, average='weighted'), 3),
          'f1_score_micro': round(f1_score(y_test, y_pred, average='micro'), 3),
          'f1_score_macro': round(f1_score(y_test, y_pred, average='macro'), 3),
          'f1_score_weighted': round(f1_score(y_test, y_pred, average='weighted'), 3),
      }

    return classifier, metrics, conf_mat, precision_recall

def train_LinearSVC_Model_Kfold(X, y, n_splits = 5, penalty='l2', loss='hinge', C=1.0, max_iter=100):

    """
    Implements training and testing of LinearSVC model with K-Fold cross validation.

    Parameters:
        X: DataFrame of X features.
        y: Series of y variable.
        n_splits: int representing number of folds, default = 5.
        parameters that can be passed to LinearSVC function are added as optional parameters.
        check LinearSVC official documentation for more details.

    Returns:
        classifier: trained LinearSVC classifier.
        metrics: dictionary of evaluation metrics.
        conf_mat: ndarray of confusion matrix.
        precision_recall: list of precision-recall curve data. (None for kfold)

    """

    # Create classifier
    classifier = LinearSVC(penalty=penalty, loss=loss, max_iter=max_iter, C=C)


    accuracies = []
    balanced_accuracies = []
    fit_times = []
    predict_times = []
    confusion_matrices = []

    precisions_micro = []
    precisions_macro = []
    precisions_weighted = []
    recalls_micro = []
    recalls_macro = []
    recalls_weighted = []
    f1_scores_micro = []
    f1_scores_macro = []
    f1_scores_weighted = []

    # Perform cross-validation
    kfold = KFold(n_splits=n_splits)
    for train_index, test_index in kfold.split(X):

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Train the classifier
        start_time = time.time()
        classifier.fit(X_train, y_train)
        fit_time = time.time() - start_time
        fit_times.append(fit_time)

        # Predict on the test set
        start_time = time.time()
        y_pred = classifier.predict(X_test)
        predict_time = time.time() - start_time
        predict_times.append(predict_time)

        # Calculate remaining metrics
        accuracies.append(accuracy_score(y_test, y_pred))
        balanced_accuracies.append(balanced_accuracy_score(y_test, y_pred))
        confusion_matrices.append(confusion_matrix(y_test, y_pred))

        precisions_micro.append(precision_score(y_test, y_pred, average='micro'))
        precisions_macro.append(precision_score(y_test, y_pred, average='macro'))
        precisions_weighted.append(precision_score(y_test, y_pred, average='weighted'))
        recalls_micro.append(recall_score(y_test, y_pred, average='micro'))
        recalls_macro.append(recall_score(y_test, y_pred, average='macro'))
        recalls_weighted.append(recall_score(y_test, y_pred, average='weighted'))
        f1_scores_micro.append(f1_score(y_test, y_pred, average='micro'))
        f1_scores_macro.append(f1_score(y_test, y_pred, average='macro'))
        f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted'))

    # Calculate average evaluation metrics

    conf_mat = np.round(np.mean(confusion_matrices, axis=0)).astype(int)
    precision_recall = None

    metrics = {
        'type': 'kfold',
        'n_classes':len(set(y)),
        'accuracy': round(np.mean(accuracies) * 100, 2),
        'balanced_accuracy': round(np.mean(balanced_accuracies) * 100, 2),
        'fit_time': round(np.sum(fit_times)  * 1000),
        'predict_time': round(np.mean(predict_times)  * 1000),
        'misclassified': np.sum(conf_mat) - np.sum(np.diag(conf_mat)),
        'precision_micro': round(np.mean(precisions_micro), 3),
        'precision_macro': round(np.mean(precisions_macro), 3),
        'precision_weighted': round(np.mean(precisions_weighted), 3),
        'recall_micro': round(np.mean(recalls_weighted), 3),
        'recall_macro': round(np.mean(recalls_weighted), 3),
        'recall_weighted': round(np.mean(recalls_weighted), 3),
        'f1_score_micro': round(np.mean(f1_scores_weighted), 3),
        'f1_score_macro': round(np.mean(f1_scores_weighted), 3),
        'f1_score_weighted': round(np.mean(f1_scores_weighted), 3),
    }


    return classifier, metrics, conf_mat,precision_recall

def print_metrics(metrics, conf_mat, precision_recall):
    """
    Prints evaluation metrics and confusion matrix of model trained with train_LinearSVC_Model() function.

    Parameters:
        metrics: dictionary of evaluation metrics returned by train_LinearSVC_Model() function.
        conf_mat: ndarray of confusion matrix returned by train_LinearSVC_Model() function.
        precision_recall: list of precision-recall curve data.

    """

  # Print evaluation metrics

    print(f"Accuracy: {metrics['accuracy']}%")
    print(f"Precision\n\tMicro: {metrics['precision_micro']}\n\tMacro: {metrics['precision_macro']}\n\tWeighted: {metrics['precision_weighted']}")
    print(f"Recall\n\tMicro: {metrics['recall_micro']}\n\tMacro: {metrics['recall_macro']}\n\tWeighted: {metrics['recall_weighted']}")
    print(f"F1 score\n\tMicro: {metrics['f1_score_micro']}\n\tMacro: {metrics['f1_score_macro']}\n\tWeighted: {metrics['f1_score_weighted']}")
    print(f"Fit time: {metrics['fit_time']} ms")
    print(f"Predict time: {metrics['predict_time']} ms")

    print()
    print(f"Number of misclassified: {metrics['misclassified']}")

    # Plot confusion matrix and pr curve
    if(metrics['type'] == 'train-test'):
        fig = plt.figure(figsize=(11,5), layout='constrained')
        gs = fig.add_gridspec(1,2)

        ax0 = fig.add_subplot(gs[0,0])
        sns.heatmap(conf_mat, annot=True, fmt='d', ax=ax0)
        ax0.set_title('Confusion Matrix')
        ax0.set_xlabel('Predicted')
        ax0.set_ylabel('True')

        precision = precision_recall[0]
        recall = precision_recall[1]
        average_precision = precision_recall[2]

        colors = sns.color_palette('colorblind')

        ax1 = fig.add_subplot(gs[0,1])

        for i, color in zip(range(metrics['n_classes']), colors):
            ax1.plot(recall[i], precision[i], color=color, lw=2.5, label='Class {0} (area = {1:0.2f})'.format(i, average_precision[i]))

        ax1.set_xlim([0.0, 1.0])
        ax1.set_ylim([0.0, 1.5])
        ax1.legend(bbox_to_anchor=(-0.1, 1.1), loc='upper left')
        ax1.set_xlabel('Recall')
        ax1.set_ylabel('Precision')
        ax1.set_title('Precision_recall Curve')

    else:
        plt.figure(figsize=(7, 6))
        sns.heatmap(conf_mat, annot=True, fmt='d')

    plt.show()

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.svm import LinearSVC


def fine_tune_LinearSVC(X, y, test_size=0.2, random_state=None, cv=5, scoring='accuracy', n_iter=200):
  """
  Performs hyperparameter tuning for LinearSVC model using RandomizedSearchCV with data splitting.

  Parameters:
      X: DataFrame of X features.
      y: Series of y variable.
      test_size: Proportion of data for the test set (default: 0.2).
      random_state: Seed for random splitting (default: None).
      cv: Number of folds for cross-validation (default: 5).
      scoring: Evaluation metric to use for scoring (default: 'accuracy').
      n_iter: Number of iterations for randomized search (default: 100).

  Returns:
      best_params: Dictionary containing the best hyperparameters found (C, loss, tol).
  """

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

  # Define hyperparameter distributions
  param_dist = {
      'C': [0.001, 0.01, 0.1, 1, 10, 100],
      'loss': ["hinge", "squared_hinge"],
      #'tol': [1e-3, 1e-4, 1e-5]
  }

  clf = LinearSVC()
  search = RandomizedSearchCV(clf, param_distributions=param_dist, cv=cv, scoring=scoring, n_iter=n_iter, random_state=random_state)

  search.fit(X_train, y_train)

  # Access best parameters
  best_params = search.best_params_

  return best_params

class_counts = df['Cat'].value_counts()
print(class_counts)

"""## Category Classification

### Without dimentionality reduction
"""

print("Training model without dimentionality reduction..")
classifier, metrics, conf_mat, precision_recall= train_LinearSVC_Model(preprocessed_data, df['Cat'])

print_metrics(metrics, conf_mat, precision_recall)

"""### LDA Only - Without Cross Validation

#### Category classification with Subcategory LDA components
"""

print("Training model with Subcategory LDA features..")
classifier, metrics, conf_mat, precision_recall = train_LinearSVC_Model(lda_features_subcategory, df['Cat'])
metrics_lda_subcategories_m=metrics

metrics

print_metrics(metrics, conf_mat,precision_recall)

"""#### Category classification with Category LDA components"""

print("Training model with Category LDA features..")
classifier, metrics, conf_mat, precision_recall = train_LinearSVC_Model(lda_features_category, df['Cat'])
metrics_lda_categoris_m=metrics

print_metrics(metrics, conf_mat, precision_recall)

"""### LDA Only - With Cross Validation

#### Category classification with Category LDA components and Cross Validation

>
"""

print("Training model with Category LDA features and cross validation..")
classifier, metrics, conf_mat, precision_recall = train_LinearSVC_Model_Kfold(lda_features_category, df['Cat'])
metrics_lda_categories_cross_m=metrics

print_metrics(metrics, conf_mat, precision_recall)

"""By trying to use Cross Validation, Category LDA is the only model where we can notice an improve in accuracy. However, fit time also increaded due to using cross validation.

### Combining PCA and LDA

#### First 11 components (best from skree plot)

#### Category Classification with 11 PCA components and LDA using Subcategory column
"""

pca_lda_data = perform_pca_lda(y=df['Sub_Cat'], n_components=11)
print_pca_lda_results(pca_lda_data)

print("Training model with 11 PCA features and Subcategory LDA..")
classifier, metrics, conf_mat, precision_recall = train_LinearSVC_Model(pca_lda_data, df['Cat'])
metrics_pca11_subcategories_m=metrics

print_metrics(metrics, conf_mat, precision_recall)

"""#### Category Classification with 11 PCA components and LDA using Category column"""

pca_lda_data = perform_pca_lda(y=df['Cat'], n_components=11)
print_pca_lda_results(pca_lda_data)

print("Training model with 11 PCA features and Subcategory LDA..")
classifier, metrics, conf_mat, precision_recall = train_LinearSVC_Model(pca_lda_data, df['Cat'])
metrics_pca11_categories_m=metrics

print_metrics(metrics, conf_mat, precision_recall)

"""We can see that PCA doesn't affect the performance of Subcategory LDA, but improves the performance of Category LDA.

By experimenting with different PCA values, we can see that increasing the number of components would give higher accuracy but also higher training time, and decreasing the number of components would give smaller training time but worse accuracy. 11 components is the ideal balance between accuracy and fit time.
"""

metrics_data_m = {
    "Model": ["Classification with Subcategory LDA components", "Classification with Category LDA components", "Category classification with LDA and Cross Validation",
              "Classification with 11 PCA components and LDA using Subcategory", " Classification with 11 PCA components and LDA using Category"],
}

# metrics to include
metrics_to_include_m = ['accuracy', 'precision_macro', 'recall_macro', 'f1_score_macro', 'fit_time', 'predict_time']

# add metrics to metrics_data dictionary
for metric in metrics_to_include_m:
    metrics_data_m[metric.capitalize()] = [globals().get(f"metrics_{name}")[metric] for name in ['lda_subcategories_m', 'lda_categoris_m', 'lda_categories_cross_m', 'pca11_subcategories_m', 'pca11_categories_m']]

df_metrics_m = pd.DataFrame(metrics_data_m)

df_metrics_m.columns.tolist()

# draw comparaison table
df_metrics_m.sort_values(by="Accuracy", ascending=False, inplace=True)
comparison_table = df_metrics_m.style.background_gradient(cmap='YlOrBr').set_properties(**{
    'font-family': 'Lucida Calligraphy',
    'color': 'neon',
    'font-size': '15px'
})
comparison_table

# Drop two models with smallest balanced accuracy
# Get index of rows
dropped_rows_indices = df_metrics_m.tail(1).index.tolist()

dropped_rows = df_metrics_m.loc[dropped_rows_indices]  # Access rows by index list

print("Dropped rows:")
print(dropped_rows.to_string())  # Print dropped rows as string for better formatting

# Remove rows
new_df = df_metrics_m.iloc[:-1]

comparison_table = new_df[['Model','Accuracy']].style.background_gradient(cmap='GnBu').set_properties(**{
  'font-family': 'Lucida Calligraphy',
  'color': 'neon',
  'font-size': '15px'
})

print("\nComparison Table:")
comparison_table

df_metrics_m['Accuracy'] /= 100

data_list = []

# Iterate over each row in the original DataFrame and append the transformed data
for index, row in new_df.iterrows():
    model = row['Model']
    accuracy = row['Accuracy']
    precision= row['Precision_macro']
    recall = row['Recall_macro']
    f1_score=row['F1_score_macro']

    data_list.append({'Model': model, 'Measures': 'Accuracy', 'Scores': accuracy	})

    data_list.append({'Model': model, 'Measures': 'Precision', 'Scores': precision})

    data_list.append({'Model': model, 'Measures': 'Recall', 'Scores': recall})

    data_list.append({'Model': model, 'Measures': 'F1_Score', 'Scores': f1_score})


df_transformed = pd.DataFrame(data_list)
df_transformed = df_transformed.sort_values(by=['Measures', 'Model']).reset_index(drop=True)


df_transformed

sns.set(rc={"figure.figsize":(10, 6)})

sns.barplot(data=df_transformed, y='Measures', x='Scores', hue='Model')

plt.title('Model Performance', fontdict={'fontsize': 15, 'color': 'blue'})
plt.xlabel('Scores', fontdict={'fontsize': 14, 'color': 'green'})
plt.ylabel('Measures', fontdict={'fontsize': 14, 'color': 'green'})

plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize='10', title='Models')
plt.show()

# Define the model value to drop
model_to_drop = "Classification with 11 PCA components and LDA using Subcategory"

filtered_df = new_df.drop(new_df[new_df["Model"] == model_to_drop].index)

filtered_df

import matplotlib.pyplot as plt

classifiers = filtered_df['Model']
Fit_time = filtered_df['Fit_time']


classifiers2 = filtered_df['Model']
Predict_time2 = filtered_df['Predict_time']



fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

y = range(len(classifiers))


bars1 = ax1.barh(y, Fit_time, color='#f4d35e')

ax1.set_xlabel('Fit Time (s)')
ax1.set_title('Model Fit Time')
ax1.set_yticks(y)
ax1.set_yticklabels(classifiers)
ax1.invert_yaxis()  # Labels read top-to-bottom

for bar in bars1:
    width = bar.get_width()
    label_x_pos = width if width > 0 else width - 0.05
    ax1.text(label_x_pos, bar.get_y() + bar.get_height() / 2, f'{width:.2f}',
             va='center', ha='right' if width > 0 else 'left', color='blue')


bars2 = ax2.barh(y, Predict_time2, color='#fecb52')

ax2.set_xlabel('predict Time (s)')
ax2.set_title('Model Predict Time')
ax2.set_yticks(y)
ax2.set_yticklabels(classifiers2)
ax2.invert_yaxis()

for bar in bars2:
    width = bar.get_width()
    label_x_pos = width if width > 0 else width - 0.05
    ax2.text(label_x_pos, bar.get_y() + bar.get_height() / 2, f'{width:.2f}',
             va='center', ha='right' if width > 0 else 'left', color='blue')

plt.subplots_adjust(left=0.1, right=0.9, wspace=1.7)

plt.show()

"""##After observing the plot of Fit time and We can Conclude that Classification with Subcategory LDA components is the best ##"""